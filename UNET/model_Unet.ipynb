{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Kết nối drive và import các thư viện cần thiết"
      ],
      "metadata": {
        "id": "TwLPUD2yEbU6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRPwttSqRkMZ",
        "outputId": "8400de7b-2f1c-47f9-e3a7-ddaeaa51fb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "feqzl2wYTL3f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lấy data"
      ],
      "metadata": {
        "id": "o2aj-pncEbQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SIZE = 256"
      ],
      "metadata": {
        "id": "xY1iG6MaahrT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(X_path, y_path):\n",
        "  arr = []\n",
        "  for i in os.listdir(X_path):\n",
        "    arr.append(i[:-4])\n",
        "  list_X = []\n",
        "  list_y = []\n",
        "  for i in arr:\n",
        "    list_X.append(X_path+'/'+i+'.jpg')\n",
        "    list_y.append(y_path+'/'+i+'.png')\n",
        "  return list_X, list_y"
      ],
      "metadata": {
        "id": "kZWdo2l_cPIO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Thay đổi các đường dẫn đề phù hợp với thiết bị\n",
        "images_train_path1, masks_train_path1 = parse_data('/content/drive/MyDrive/ProjectCS406/dataset/train/photos','/content/drive/MyDrive/ProjectCS406/dataset/train/masks')\n",
        "images_train_path2, masks_train_path2 = parse_data('/content/drive/MyDrive/ProjectCS406/augment_data/train/photos', '/content/drive/MyDrive/ProjectCS406/augment_data/train/masks')\n",
        "\n",
        "images_train_path = images_train_path1 + images_train_path2\n",
        "masks_train_path = masks_train_path1 + masks_train_path2"
      ],
      "metadata": {
        "id": "NRRlpqQJcVn0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Thay đổi các đường dẫn đề phù hợp với thiết bị\n",
        "images_val_path1, masks_val_path1 = parse_data('/content/drive/MyDrive/ProjectCS406/dataset/val/photos','/content/drive/MyDrive/ProjectCS406/dataset/val/masks')\n",
        "images_val_path2, masks_val_path2 = parse_data('/content/drive/MyDrive/ProjectCS406/augment_data/val/photos','/content/drive/MyDrive/ProjectCS406/augment_data/val/masks')\n",
        "\n",
        "images_val_path = images_val_path1 + images_val_path2\n",
        "masks_val_path = masks_val_path1 + masks_val_path2\n"
      ],
      "metadata": {
        "id": "e8UaySjRdedM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Đọc ảnh từ danh sách path\n",
        "images_train = [cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2GRAY) for img in images_train_path]\n",
        "masks_train = [cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2GRAY) for img in masks_train_path]\n",
        "images_val = [cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2GRAY) for img in images_val_path]\n",
        "masks_val = [cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2GRAY) for img in masks_val_path]"
      ],
      "metadata": {
        "id": "7Hu3oObudkW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## covert danh sách ảnh qua dạng numpy array\n",
        "images_train = np.array(images_train)\n",
        "images_train = np.expand_dims(images_train, axis=3)\n",
        "\n",
        "masks_train = np.array(masks_train)\n",
        "masks_train = np.expand_dims(masks_train, axis=3)\n",
        "\n",
        "images_val = np.array(images_val)\n",
        "images_val = np.expand_dims(images_val, axis=3)\n",
        "\n",
        "masks_val = np.array(masks_val)\n",
        "masks_val = np.expand_dims(masks_val, axis=3)"
      ],
      "metadata": {
        "id": "McAchQZFeRXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Image data shape is: \", images_val.shape)\n",
        "print(\"Mask data shape is:\", masks_val.shape)\n",
        "print(\"Max pixel value in image is: \", images_val.max())\n",
        "print(\"Labels in the mask are: \", np.unique(masks_val))"
      ],
      "metadata": {
        "id": "UIhTABsjev-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizations iamges"
      ],
      "metadata": {
        "id": "nPg5jesjEy1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize images\n",
        "images_train = images_train/255\n",
        "\n",
        "masks_train = masks_train/255\n",
        "\n",
        "images_val = images_val/255\n",
        "\n",
        "masks_val = masks_val/255\n"
      ],
      "metadata": {
        "id": "EESkwkMeiHqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build model "
      ],
      "metadata": {
        "id": "T5FuDIOtEvOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, Concatenate, Dropout, MaxPooling2D, Input, BatchNormalization, Lambda, Activation, concatenate, Conv2DTranspose, MaxPool2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def conv_block(input, num_filters):\n",
        "  x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def encoder_block(input, num_filters):\n",
        "  x = conv_block(input, num_filters)\n",
        "  p = MaxPool2D((2,2))(x)\n",
        "  return x, p\n",
        "\n",
        "def decoder_block(input, skip_features, num_filters):\n",
        "  x = Conv2DTranspose(num_filters, (2,2), strides=2, padding=\"same\")(input)\n",
        "  x = Concatenate()([x, skip_features])\n",
        "  x = conv_block(x, num_filters)\n",
        "  return x\n",
        "\n",
        "def build_unet(input_shape, n_classes):\n",
        "  inputs = Input(input_shape)\n",
        "\n",
        "  s1,p1 = encoder_block(inputs, 64)\n",
        "  s2,p2 = encoder_block(p1, 128)\n",
        "  s3,p3 = encoder_block(p2, 256)\n",
        "  s4,p4 = encoder_block(p3, 512)\n",
        "\n",
        "  b1 = conv_block(p4, 1024)\n",
        "\n",
        "  d1 = decoder_block(b1, s4, 512)\n",
        "  d2 = decoder_block(d1, s3, 256)\n",
        "  d3 = decoder_block(d2, s2, 128)\n",
        "  d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "  if n_classes == 1:\n",
        "    activation = 'sigmoid'\n",
        "  else:\n",
        "    activation = 'softmax'\n",
        "\n",
        "  outputs = Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d4)\n",
        "  print(activation)\n",
        "  \n",
        "  model = Model(inputs, outputs, name=\"U-Net\")\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "Uf6XsAfZiria"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup các parameter"
      ],
      "metadata": {
        "id": "XVtsM47mOghK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_HEIGHT = images_train.shape[1]\n",
        "IMAGE_WIDTH = images_train.shape[2]\n",
        "IMAGE_CHANNELS = images_train.shape[3]\n",
        "\n",
        "input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)"
      ],
      "metadata": {
        "id": "81ZfGrrimjg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_unet(input_shape, n_classes=1)\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hsupmSolm8u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiến hành training"
      ],
      "metadata": {
        "id": "-wSyH3euOmVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/ProjectCS406/New/checkpoint0/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                              save_weights_only=True,\n",
        "                                              period=25,\n",
        "                                              verbose=1)\n",
        "\n",
        "history = model.fit(images_train, masks_train,\n",
        "                    batch_size=16, \n",
        "                    verbose=1,\n",
        "                    epochs=100,\n",
        "                    callbacks=[cp_callback],\n",
        "                    validation_data=(images_val, masks_val),\n",
        "                    shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RI4M2uZwoK5i",
        "outputId": "c0ac65b1-11c5-4200-d2d3-4dd714ee2006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "53/53 [==============================] - 73s 1s/step - loss: 0.4131 - accuracy: 0.8188 - val_loss: 93.4089 - val_accuracy: 0.6628\n",
            "Epoch 2/100\n",
            "53/53 [==============================] - 45s 851ms/step - loss: 0.3296 - accuracy: 0.8614 - val_loss: 0.9784 - val_accuracy: 0.6477\n",
            "Epoch 3/100\n",
            "53/53 [==============================] - 46s 869ms/step - loss: 0.2975 - accuracy: 0.8770 - val_loss: 4.0522 - val_accuracy: 0.6630\n",
            "Epoch 4/100\n",
            "53/53 [==============================] - 46s 875ms/step - loss: 0.2713 - accuracy: 0.8871 - val_loss: 5.2298 - val_accuracy: 0.6628\n",
            "Epoch 5/100\n",
            "53/53 [==============================] - 47s 892ms/step - loss: 0.2635 - accuracy: 0.8895 - val_loss: 1.6149 - val_accuracy: 0.6682\n",
            "Epoch 6/100\n",
            "53/53 [==============================] - 47s 895ms/step - loss: 0.2552 - accuracy: 0.8933 - val_loss: 0.7609 - val_accuracy: 0.7416\n",
            "Epoch 7/100\n",
            "53/53 [==============================] - 48s 903ms/step - loss: 0.2432 - accuracy: 0.8986 - val_loss: 0.5710 - val_accuracy: 0.8136\n",
            "Epoch 8/100\n",
            "53/53 [==============================] - 48s 902ms/step - loss: 0.2340 - accuracy: 0.9032 - val_loss: 0.7107 - val_accuracy: 0.7722\n",
            "Epoch 9/100\n",
            "53/53 [==============================] - 47s 897ms/step - loss: 0.2260 - accuracy: 0.9071 - val_loss: 0.4450 - val_accuracy: 0.8546\n",
            "Epoch 10/100\n",
            "53/53 [==============================] - 48s 902ms/step - loss: 0.2180 - accuracy: 0.9100 - val_loss: 0.3469 - val_accuracy: 0.8778\n",
            "Epoch 11/100\n",
            "53/53 [==============================] - 48s 904ms/step - loss: 0.2099 - accuracy: 0.9144 - val_loss: 0.3426 - val_accuracy: 0.8925\n",
            "Epoch 12/100\n",
            "53/53 [==============================] - 48s 904ms/step - loss: 0.2044 - accuracy: 0.9163 - val_loss: 0.3302 - val_accuracy: 0.8992\n",
            "Epoch 13/100\n",
            "53/53 [==============================] - 50s 937ms/step - loss: 0.1986 - accuracy: 0.9183 - val_loss: 0.4659 - val_accuracy: 0.8639\n",
            "Epoch 14/100\n",
            "53/53 [==============================] - 48s 909ms/step - loss: 0.1909 - accuracy: 0.9220 - val_loss: 0.3470 - val_accuracy: 0.8897\n",
            "Epoch 15/100\n",
            "53/53 [==============================] - 48s 907ms/step - loss: 0.1877 - accuracy: 0.9230 - val_loss: 0.4171 - val_accuracy: 0.8759\n",
            "Epoch 16/100\n",
            "53/53 [==============================] - 48s 910ms/step - loss: 0.1902 - accuracy: 0.9225 - val_loss: 0.2638 - val_accuracy: 0.8981\n",
            "Epoch 17/100\n",
            "53/53 [==============================] - 48s 908ms/step - loss: 0.1859 - accuracy: 0.9237 - val_loss: 0.3297 - val_accuracy: 0.8719\n",
            "Epoch 18/100\n",
            "53/53 [==============================] - 48s 910ms/step - loss: 0.1798 - accuracy: 0.9277 - val_loss: 0.2352 - val_accuracy: 0.9108\n",
            "Epoch 19/100\n",
            "53/53 [==============================] - 48s 912ms/step - loss: 0.1701 - accuracy: 0.9314 - val_loss: 0.2707 - val_accuracy: 0.9009\n",
            "Epoch 20/100\n",
            "53/53 [==============================] - 48s 912ms/step - loss: 0.1629 - accuracy: 0.9344 - val_loss: 0.2467 - val_accuracy: 0.9028\n",
            "Epoch 21/100\n",
            "53/53 [==============================] - 50s 938ms/step - loss: 0.1612 - accuracy: 0.9351 - val_loss: 0.2461 - val_accuracy: 0.9015\n",
            "Epoch 22/100\n",
            "53/53 [==============================] - 49s 932ms/step - loss: 0.1596 - accuracy: 0.9362 - val_loss: 0.2468 - val_accuracy: 0.9045\n",
            "Epoch 23/100\n",
            "53/53 [==============================] - 49s 932ms/step - loss: 0.1515 - accuracy: 0.9395 - val_loss: 0.2253 - val_accuracy: 0.9191\n",
            "Epoch 24/100\n",
            "53/53 [==============================] - 50s 937ms/step - loss: 0.1467 - accuracy: 0.9421 - val_loss: 0.1731 - val_accuracy: 0.9330\n",
            "Epoch 25/100\n",
            "53/53 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9443"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lưu model"
      ],
      "metadata": {
        "id": "U3YskVPUE6Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/ProjectCS406/New/model_unet_aug.hdf5')"
      ],
      "metadata": {
        "id": "TAbtE4RwveOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}